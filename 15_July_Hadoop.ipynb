{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e91fc3b",
   "metadata": {},
   "source": [
    "## Python program to read a Hadoop configuration file and display the core components of Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8caefe",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00508cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hadoop.conf')\n",
    "\n",
    "# Retrieve the core components of Hadoop\n",
    "core_components = config.get('core-site', 'fs.defaultFS')\n",
    "\n",
    "# Display the core components of Hadoop\n",
    "print(\"Core components of Hadoop:\")\n",
    "print(core_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f73dd",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43073e",
   "metadata": {},
   "source": [
    "This program uses the configparser module to read a Hadoop configuration file (hadoop.conf). It retrieves the value of the fs.defaultFS property from the [core-site] section, which represents the core components of Hadoop. Finally, it prints the core components to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323aa39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161093a5",
   "metadata": {},
   "source": [
    "## 2. Python function to calculate the total file size in a Hadoop Distributed File System (HDFS) directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6f97c",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39141543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    # Run the hdfs dfs -du command to get the directory size\n",
    "    command = ['hdfs', 'dfs', '-du', '-s', '-h', directory]\n",
    "    result = subprocess.check_output(command).decode().strip().split('\\t')\n",
    "\n",
    "    # Extract the size value from the command output\n",
    "    size = result[0]\n",
    "\n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "directory = '/user/example'\n",
    "total_size = get_directory_size(directory)\n",
    "print(f\"Total file size in {directory}: {total_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745c59",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca2d16",
   "metadata": {},
   "source": [
    "This Python function uses the subprocess module to run the hdfs dfs -du -s -h command, which calculates the total size of a directory in the Hadoop Distributed File System (HDFS). The function takes a directory path as input, executes the command using subprocess.check_output(), and extracts the size value from the command output. The function returns the total file size as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025c3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111c8c74",
   "metadata": {},
   "source": [
    "## Python program to extract and display the top N most frequent words from a large text file using the MapReduce approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba68a7",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6034533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import multiprocessing\n",
    "\n",
    "def mapper(chunk):\n",
    "    # Split the chunk into words and count their occurrences\n",
    "    words = chunk.split()\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Emit each word with its count\n",
    "    return word_counts.items()\n",
    "\n",
    "def reducer(mapped_items):\n",
    "    # Aggregate the counts of the same word across different chunks\n",
    "    word_counts = Counter()\n",
    "    for item in mapped_items:\n",
    "        word, count = item\n",
    "        word_counts[word] += count\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def get_top_n_words(filename, n):\n",
    "    # Read the file and split it into chunks\n",
    "    with open(filename, 'r') as file:\n",
    "        chunks = file.read().split('\\n')\n",
    "\n",
    "    # Process the chunks in parallel using multiple processes\n",
    "    pool = multiprocessing.Pool()\n",
    "    mapped_items = pool.map(mapper, chunks)\n",
    "    word_counts = reducer(mapped_items)\n",
    "\n",
    "    # Get the top N most frequent words\n",
    "    top_n_words = word_counts.most_common(n)\n",
    "\n",
    "    return top_n_words\n",
    "\n",
    "# Example usage\n",
    "filename = 'large_text_file.txt'\n",
    "n = 10\n",
    "top_words = get_top_n_words(filename, n)\n",
    "print(f\"Top {n} most frequent words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b570a3",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c482bb2",
   "metadata": {},
   "source": [
    "This program demonstrates a simple MapReduce approach to extract and display the top N most frequent words from a large text file. The mapper function takes a chunk of text, splits it into words, and counts their occurrences using the Counter class. It returns the word-count pairs. The reducer function aggregates the word counts from different chunks. The get_top_n_words function reads the large text file, splits it into chunks, and processes them in parallel using multiple processes. It applies the mapper function to each chunk, then reduces the mapped items using the reducer function. Finally, it retrieves the top N most frequent words using the most_common method of the Counter object and returns them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e7026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8ade3d",
   "metadata": {},
   "source": [
    "## Python script to check the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b648c",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_node_health(node_type, hostname, port):\n",
    "    # Make a GET request to the Hadoop REST API\n",
    "    url = f\"http://{hostname}:{port}/jmx?qry=Hadoop:service=NameNode,name={node_type}Info\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Extract the health status from the API response\n",
    "    health_status = response.json()['beans'][0]['State']\n",
    "\n",
    "    return health_status\n",
    "\n",
    "# Example usage\n",
    "namenode_hostname = 'namenode.example.com'\n",
    "datanode_hostname = 'datanode.example.com'\n",
    "port = 50070\n",
    "\n",
    "namenode_status = get_node_health('NameNode', namenode_hostname, port)\n",
    "datanode_status = get_node_health('DataNode', datanode_hostname, port)\n",
    "\n",
    "print(f\"NameNode status: {namenode_status}\")\n",
    "print(f\"DataNode status: {datanode_status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff8828",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ad934",
   "metadata": {},
   "source": [
    "This Python script uses the requests module to make a GET request to the Hadoop NameNode and DataNode REST APIs. The get_node_health function takes the node type (e.g., 'NameNode' or 'DataNode'), hostname, and port as input. It constructs the URL for the respective API endpoint and retrieves the health status from the response. Finally, the script calls the get_node_health function for the NameNode and DataNode, and prints their respective health statuses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ea9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82c21771",
   "metadata": {},
   "source": [
    "## Python program to list all the files and directories in a specific HDFS path:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597e894",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab896175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    # Run the hdfs dfs -ls command to list the files and directories in the HDFS path\n",
    "    command = ['hdfs', 'dfs', '-ls', path]\n",
    "    result = subprocess.check_output(command).decode().strip().split('\\n')\n",
    "\n",
    "    # Extract the file and directory names from the command output\n",
    "    items = [line.split()[-1] for line in result[1:]]\n",
    "\n",
    "    return items\n",
    "\n",
    "# Example usage\n",
    "hdfs_path = '/user/example'\n",
    "items = list_hdfs_path(hdfs_path)\n",
    "print(f\"Files and directories in {hdfs_path}:\")\n",
    "for item in items:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759eb479",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5d49",
   "metadata": {},
   "source": [
    "This Python program uses the subprocess module to run the hdfs dfs -ls command, which lists all the files and directories in a specific HDFS path. The program takes the HDFS path as input, executes the command using subprocess.check_output(), and extracts the file and directory names from the command output. Finally, it prints the list of files and directories to the console.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746c6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "248bc929",
   "metadata": {},
   "source": [
    "## Python program to analyze the storage utilization of DataNodes in a Hadoop cluster and identify the nodes with the highest and lowest storage capacities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecec2d",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_storage_utilization(hostname, port):\n",
    "    # Make a GET request to the Hadoop REST API\n",
    "    url = f\"http://{hostname}:{port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Extract the storage utilization information from the API response\n",
    "    data = response.json()['beans'][0]\n",
    "    total_capacity = data['Capacity']\n",
    "    used_capacity = data['Used']\n",
    "    remaining_capacity = data['Remaining']\n",
    "\n",
    "    # Find the DataNode with the highest and lowest storage capacities\n",
    "    datanodes = data['DatanodeVolumeInfo']\n",
    "    datanodes.sort(key=lambda x: x['usedSpace'], reverse=True)\n",
    "    highest_capacity_node = datanodes[0]['storageID']\n",
    "    lowest_capacity_node = datanodes[-1]['storageID']\n",
    "\n",
    "    return total_capacity, used_capacity, remaining_capacity, highest_capacity_node, lowest_capacity_node\n",
    "\n",
    "# Example usage\n",
    "datanode_hostname = 'datanode.example.com'\n",
    "port = 50075\n",
    "\n",
    "total_capacity, used_capacity, remaining_capacity, highest_capacity_node, lowest_capacity_node = analyze_storage_utilization(datanode_hostname, port)\n",
    "\n",
    "print(f\"Total capacity: {total_capacity}\")\n",
    "print(f\"Used capacity: {used_capacity}\")\n",
    "print(f\"Remaining capacity: {remaining_capacity}\")\n",
    "print(f\"Highest capacity node: {highest_capacity_node}\")\n",
    "print(f\"Lowest capacity node: {lowest_capacity_node}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5f50b",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52880e19",
   "metadata": {},
   "source": [
    "This Python program uses the requests module to make a GET request to the Hadoop DataNode REST API. The analyze_storage_utilization function takes the DataNode hostname and port as input. It constructs the URL for the DataNode API endpoint and retrieves the storage utilization information from the response. The program extracts the total capacity, used capacity, and remaining capacity from the API response. It also analyzes the storage capacities of individual DataNodes by sorting them based on their used space. Finally, the program returns the total capacity, used capacity, remaining capacity, and identifies the DataNode with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295c8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "577e4763",
   "metadata": {},
   "source": [
    "##  Python script to interact with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bffa3e",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(job_data, resource_manager_url):\n",
    "    # Submit the Hadoop job using the YARN REST API\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(url)\n",
    "    application_id = response.json()['application-id']\n",
    "\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "    response = requests.post(url, json=job_data)\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def monitor_job_status(application_id, resource_manager_url):\n",
    "    # Monitor the progress of the Hadoop job\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        status = response.json()['appAttempts']['appAttempt'][0]['state']\n",
    "        if status != 'RUNNING':\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(application_id, resource_manager_url):\n",
    "    # Retrieve the final output of the Hadoop job\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
    "    response = requests.get(url)\n",
    "    output = response.json()['appAttempts']['appAttempt'][0]['appAttemptId']\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "job_data = {\n",
    "    \"application-id\": \"my-hadoop-job\",\n",
    "    \"application-name\": \"My Hadoop Job\",\n",
    "    # Additional job configuration parameters...\n",
    "}\n",
    "\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "\n",
    "application_id = submit_hadoop_job(job_data, resource_manager_url)\n",
    "print(f\"Job submitted. Application ID: {application_id}\")\n",
    "\n",
    "monitor_job_status(application_id, resource_manager_url)\n",
    "print(\"Job completed.\")\n",
    "\n",
    "output = retrieve_job_output(application_id, resource_manager_url)\n",
    "print(f\"Job output: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d193f",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2224fa9",
   "metadata": {},
   "source": [
    "This Python script interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output. The submit_hadoop_job function submits the Hadoop job by making a POST request to the /ws/v1/cluster/apps/new-application and /ws/v1/cluster/apps/{application_id}/app endpoints. It returns the application ID assigned to the submitted job. The monitor_job_status function continuously checks the job status by making a GET request to the /ws/v1/cluster/apps/{application_id}/appattempts endpoint until the job completes. The retrieve_job_output function retrieves the final output of the job by making a GET request to the /ws/v1/cluster/apps/{application_id}/appattempts endpoint. The example usage section demonstrates how to use these functions and print relevant information about the submitted Hadoop job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f3bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa6d9b2",
   "metadata": {},
   "source": [
    "## Python script to interact with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8cc62a",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(job_data, resource_manager_url):\n",
    "    # Submit the Hadoop job using the YARN REST API\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(url)\n",
    "    application_id = response.json()['application-id']\n",
    "\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "    response = requests.post(url, json=job_data)\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def monitor_resource_usage(application_id, resource_manager_url):\n",
    "    # Monitor the resource usage of the Hadoop job\n",
    "    url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/metrics\"\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        metrics = response.json()['clusterMetrics']\n",
    "        allocated_memory = metrics['allocatedMB']\n",
    "        allocated_vcores = metrics['allocatedVirtualCores']\n",
    "        print(f\"Allocated memory: {allocated_memory} MB\")\n",
    "        print(f\"Allocated vCores: {allocated_vcores}\")\n",
    "        if metrics['appsSubmitted'] == metrics['appsCompleted']:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "# Example usage\n",
    "job_data = {\n",
    "    \"application-id\": \"my-hadoop-job\",\n",
    "    \"application-name\": \"My Hadoop Job\",\n",
    "    \"am-resource\": {\n",
    "        \"memory\": 2048,\n",
    "        \"vCores\": 2\n",
    "    },\n",
    "    # Additional job configuration parameters...\n",
    "}\n",
    "\n",
    "resource_manager_url = 'http://localhost:8088'\n",
    "\n",
    "application_id = submit_hadoop_job(job_data, resource_manager_url)\n",
    "print(f\"Job submitted. Application ID: {application_id}\")\n",
    "\n",
    "monitor_resource_usage(application_id, resource_manager_url)\n",
    "print(\"Resource usage monitoring completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38235791",
   "metadata": {},
   "source": [
    "### EXPLAINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf27f05",
   "metadata": {},
   "source": [
    "This Python script interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution. The submit_hadoop_job function submits the Hadoop job by making a POST request to the /ws/v1/cluster/apps/new-application and /ws/v1/cluster/apps/{application_id}/app endpoints. It returns the application ID assigned to the submitted job. The monitor_resource_usage function continuously checks the resource usage of the job by making a GET request\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c8cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c688a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1fc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e2c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
